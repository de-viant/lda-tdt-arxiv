{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:45:25.069368Z",
     "start_time": "2018-11-23T22:45:24.957182Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, FloatRangeSlider\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pprint import pprint\n",
    "from scipy import sparse\n",
    "import pyLDAvis\n",
    "from pyLDAvis.gensim import prepare\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:45:32.054802Z",
     "start_time": "2018-11-23T22:45:32.051590Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T22:45:32.554090Z",
     "start_time": "2018-11-23T22:45:32.548116Z"
    }
   },
   "outputs": [],
   "source": [
    "percentiles=np.arange(.1, 1, .1).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T20:07:02.894398Z",
     "start_time": "2018-11-10T20:07:01.942439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26717"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_path = Path('../data/clean_stop')\n",
    "text_files = text_path.glob('*.txt')\n",
    "docs = [f.read_text() for f in text_files]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:16:50.429518Z",
     "start_time": "2018-11-10T21:16:14.377458Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=.8,\n",
    "                             min_df=.05,\n",
    "                             stop_words='english',\n",
    "                             max_features=None,\n",
    "                             binary=False)\n",
    "dtm = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token per doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:16:52.989516Z",
     "start_time": "2018-11-10T21:16:52.979495Z"
    }
   },
   "outputs": [],
   "source": [
    "token_per_doc = np.array(dtm.sum(axis=1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:16:54.339211Z",
     "start_time": "2018-11-10T21:16:54.329212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    26717.000000\n",
       "mean      1973.682449\n",
       "std       1279.306551\n",
       "min          7.000000\n",
       "10%        919.000000\n",
       "20%       1213.200000\n",
       "30%       1422.000000\n",
       "40%       1590.000000\n",
       "50%       1749.000000\n",
       "60%       1931.000000\n",
       "70%       2150.000000\n",
       "80%       2504.000000\n",
       "90%       3191.000000\n",
       "max      45356.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(token_per_doc).describe(percentiles=percentiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:16:58.047654Z",
     "start_time": "2018-11-10T21:16:58.033623Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:16:58.196257Z",
     "start_time": "2018-11-10T21:16:58.162531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image          529010\n",
       "network        402964\n",
       "feature        394381\n",
       "dataset        288647\n",
       "training       279551\n",
       "learn          264218\n",
       "sample         254497\n",
       "layer          231629\n",
       "matrix         227439\n",
       "point          216743\n",
       "albeit           1814\n",
       "remarkably       1796\n",
       "involved         1782\n",
       "ghz              1780\n",
       "noticeable       1770\n",
       "devote           1765\n",
       "faculty          1747\n",
       "seminal          1743\n",
       "fortunately      1727\n",
       "terms            1445\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_count = pd.Series(np.array(dtm.sum(axis=0)).squeeze(), index=tokens).sort_values(ascending=False)\n",
    "t_count.head(10).append(t_count.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = pd.Series(tokens).to_dict()\n",
    "corpus = Sparse2Corpus(dtm, documents_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:17:03.330803Z",
     "start_time": "2018-11-10T21:17:03.328907Z"
    }
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "topic_labels = [f'Topic {i}' for i in range(1, n_topics + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:30:41.180310Z",
     "start_time": "2018-11-10T21:24:59.016294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x112ff4908>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LdaModel(corpus=corpus,\n",
    "         num_topics=n_topics,\n",
    "         id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:18:30.983022Z",
     "start_time": "2018-11-10T21:18:30.942123Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e37941f4d9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtopic_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_gensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topic_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'probability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtopic_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_gensim' is not defined"
     ]
    }
   ],
   "source": [
    "topic_words = pd.DataFrame()\n",
    "topic_probs = pd.DataFrame()\n",
    "for topic in range(n_topics):\n",
    "    terms = lda_gensim.get_topic_terms(topic)\n",
    "    top_words = pd.DataFrame(terms, columns=['term', 'probability'])\n",
    "    topic_words[topic_labels[topic]] = top_words.term.map(id2word)\n",
    "    topic_probs[topic_labels[topic]] = top_words.probability\n",
    "topic_words.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:18:33.744053Z",
     "start_time": "2018-11-10T21:18:33.695704Z"
    }
   },
   "outputs": [],
   "source": [
    "# can maybe transfer style from probabilities to terms\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "topic_probs.T.style.background_gradient(cmap=cm).format('{:,.2%}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:47:58.286796Z",
     "start_time": "2018-11-08T22:47:58.281376Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_gensim.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:18:43.826640Z",
     "start_time": "2018-11-10T21:18:39.047837Z"
    }
   },
   "outputs": [],
   "source": [
    "coherence = lda_gensim.top_topics(corpus=corpus, coherence='u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:18:45.560078Z",
     "start_time": "2018-11-10T21:18:45.375146Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_coherence  = []\n",
    "topic_words = pd.DataFrame()\n",
    "for t in range(len(coherence)):\n",
    "    label = topic_labels[t]\n",
    "    topic_coherence.append(coherence[t][1])\n",
    "    df = pd.DataFrame(coherence[t][0], columns=[(label, 'prob'), (label, 'term')])\n",
    "    df[(label, 'prob')] = df[(label, 'prob')].apply(lambda x: '{:.2%}'.format(x))\n",
    "    topic_words = pd.concat([topic_words, df], axis=1)\n",
    "                      \n",
    "topic_words.columns = pd.MultiIndex.from_tuples(topic_words.columns)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "# print(topic_words.head())\n",
    "pd.Series(topic_coherence, index=topic_labels).plot.bar(figsize=(14,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T15:49:18.828990Z",
     "start_time": "2018-11-08T15:49:18.145786Z"
    }
   },
   "source": [
    "## PyLDAVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:21:05.129411Z",
     "start_time": "2018-11-10T21:20:23.429578Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary.from_corpus(corpus, id2word)\n",
    "vis = prepare(lda_gensim, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:13:19.098095Z",
     "start_time": "2018-11-10T21:11:48.973162Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel= lda_gensim, corpus=corpus, texts=docs)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
